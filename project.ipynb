{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2124a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import tqdm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc36d995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing 11 audio files...\n",
      "✔ Processed: Speaker_0000_00009.wav\n",
      "✔ Processed: Speaker_0000_00008.wav\n",
      "✔ Processed: Speaker_0000_00005.wav\n",
      "✔ Processed: Speaker_0000_00004.wav\n",
      "✔ Processed: Speaker_0000_00010.wav\n",
      "✔ Processed: Speaker_0000_00006.wav\n",
      "✔ Processed: Speaker_0000_00007.wav\n",
      "✔ Processed: Speaker_0000_00003.wav\n",
      "✔ Processed: Speaker_0000_00002.wav\n",
      "✔ Processed: Speaker_0000_00000.wav\n",
      "✔ Processed: Speaker_0000_00001.wav\n",
      "All files processed successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class AudioProcessor:\n",
    "    def __init__(self, sample_rate=16000, duration=30, n_mels=80, n_fft=400, hop_length=160, mono=True):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.mono = mono\n",
    "        self.num_expected_samples = int(sample_rate * duration)\n",
    "        self.expected_spec_shape = (n_mels, 3000)  # Whisper expects (80, 3000) shape\n",
    "\n",
    "    def load_audio(self, file_path):\n",
    "        signal, _ = librosa.load(file_path, sr=self.sample_rate, mono=self.mono)\n",
    "        return np.pad(signal, (0, max(0, self.num_expected_samples - len(signal))), mode=\"constant\")\n",
    "\n",
    "    def extract_features(self, signal):\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=signal, sr=self.sample_rate, n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length, n_mels=self.n_mels\n",
    "        )\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)  # Convert to log scale\n",
    "        \n",
    "        # Fix shape to (80, 3000)\n",
    "        log_mel_spec = self._fix_spectrogram_shape(log_mel_spec)\n",
    "\n",
    "        # Normalize for Whisper (-1 to 1)\n",
    "        norm_feature = self._normalize(log_mel_spec)\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        return torch.tensor(norm_feature, dtype=torch.float32)\n",
    "\n",
    "    def _fix_spectrogram_shape(self, spectrogram):\n",
    "        \"\"\"Ensure the spectrogram has shape (80, 3000) by padding/truncating.\"\"\"\n",
    "        current_shape = spectrogram.shape[1]\n",
    "\n",
    "        if current_shape < self.expected_spec_shape[1]:  # Pad if too short\n",
    "            pad_width = self.expected_spec_shape[1] - current_shape\n",
    "            spectrogram = np.pad(spectrogram, ((0, 0), (0, pad_width)), mode=\"constant\")\n",
    "        elif current_shape > self.expected_spec_shape[1]:  # Truncate if too long\n",
    "            spectrogram = spectrogram[:, :self.expected_spec_shape[1]]\n",
    "\n",
    "        return spectrogram\n",
    "\n",
    "    def _normalize(self, feature):\n",
    "        \"\"\"Normalize spectrogram to range [-1, 1] (Whisper requirement).\"\"\"\n",
    "        feature = (feature - feature.min()) / (feature.max() - feature.min())  # Normalize to [0, 1]\n",
    "        return 2 * feature - 1  # Scale to range [-1, 1]\n",
    "\n",
    "    def save_data(self, feature, file_path, save_dir):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        np.save(os.path.join(save_dir, os.path.basename(file_path) + \".npy\"), feature.numpy())  # Save as NumPy array\n",
    "\n",
    "    def save_min_max(self, min_max_values, save_path):\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            pickle.dump(min_max_values, f)\n",
    "\n",
    "    def process_files(self, file_paths, feature_dir, min_max_path):\n",
    "        min_max_values = {}\n",
    "\n",
    "        if not file_paths:\n",
    "            raise FileNotFoundError(\"No audio files found! Check the FILES_DIR path.\")\n",
    "\n",
    "        print(f\" Processing {len(file_paths)} audio files...\")\n",
    "\n",
    "        for file_path in file_paths:\n",
    "            signal = self.load_audio(file_path)\n",
    "            feature_tensor = self.extract_features(signal)  # Returns a PyTorch tensor\n",
    "            self.save_data(feature_tensor, file_path, feature_dir)\n",
    "            min_max_values[file_path] = {\"min\": feature_tensor.min().item(), \"max\": feature_tensor.max().item()}\n",
    "            print(f\"✔ Processed: {os.path.basename(file_path)}\")\n",
    "\n",
    "        self.save_min_max(min_max_values, min_max_path)\n",
    "        print(\"All files processed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    FILES_DIR = \"/Users/puravgupta/Desktop/python/stt-Whisper2002/Speaker_0000/*.wav\"\n",
    "    FEATURE_DIR = \"./datasets/spectrograms/\"\n",
    "    MIN_MAX_PATH = \"./datasets/minmax/min_max_values.pkl\"\n",
    "    \n",
    "    audio_files = glob.glob(FILES_DIR, recursive=True)\n",
    "\n",
    "    processor = AudioProcessor()\n",
    "    processor.process_files(audio_files, FEATURE_DIR, MIN_MAX_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6735fccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Shape: torch.Size([80, 3000])\n",
      "Tensor Data:\n",
      " tensor([[-0.3635, -0.5583, -0.6897,  ..., -0.4162, -0.4014, -0.5088],\n",
      "        [-0.2329, -0.3242, -0.5107,  ..., -0.0677, -0.0268, -0.0108],\n",
      "        [-0.2264, -0.3017, -0.4451,  ...,  0.2028,  0.2091,  0.2257],\n",
      "        ...,\n",
      "        [-0.7672, -1.0000, -1.0000,  ..., -0.5798, -0.3596, -0.4839],\n",
      "        [-0.7760, -0.9979, -1.0000,  ..., -0.5979, -0.4271, -0.5276],\n",
      "        [-0.7870, -1.0000, -1.0000,  ..., -0.7436, -0.5475, -0.6940]])\n"
     ]
    }
   ],
   "source": [
    "processor = AudioProcessor()\n",
    "audio_path = \"/Users/puravgupta/Desktop/python/stt-Whisper2002/Speaker_0000/Speaker_0000_00001.wav\"\n",
    "signal = processor.load_audio(audio_path)\n",
    "input_tensor = processor.extract_features(signal) \n",
    "print(\"Tensor Shape:\", input_tensor.shape)\n",
    "print(\"Tensor Data:\\n\", input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69bb49ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    }
   ],
   "source": [
    "# def transcribe_audio(audio_file):\n",
    "#     \"\"\"Transcribes audio using Whisper model.\"\"\"\n",
    "#     device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#     whisper_pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\", device=device)\n",
    "#     transcription = whisper_pipe(audio_file)\n",
    "#     return transcription\n",
    "\n",
    "whisper_pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\" , device=-1)\n",
    "processor = AudioProcessor(sample_rate=16000)\n",
    "\n",
    "waveform = processor.load_audio(audio_path)\n",
    "\n",
    "waveform = np.array(waveform, dtype=np.float32)\n",
    "transcription = whisper_pipe(waveform,return_timestamps=True)\n",
    "\n",
    "transcription_data = {\n",
    "    \"audio_file\": audio_path,\n",
    "    \"transcription\": transcription[\"text\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf07473c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription saved to transcription_output.json\n"
     ]
    }
   ],
   "source": [
    "# def save_to_json(data, filename=\"transcription.json\"):\n",
    "#     \"\"\"Saves transcription output to a JSON file.\"\"\"\n",
    "#     with open(filename, \"w\") as f:\n",
    "#         json.dump(data, f, indent=4)\n",
    "#     return filename\n",
    "\n",
    "\n",
    "json_file_path = \"transcription_output.json\"\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(transcription_data, json_file, indent=4)\n",
    "\n",
    "print(f\"Transcription saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a439a8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad60d296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
